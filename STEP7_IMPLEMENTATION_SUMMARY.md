# Step 7 Implementation Summary

## âœ… Completed: Attribute-Aware Retrieval Fine-tuning (v0 Protocol Model)

All components of Step 7 have been successfully implemented according to the plan. The system is ready for training and evaluation.

---

## ğŸ“¦ Implemented Components

### 1. Core Training Infrastructure

#### **Protocol Model** (`src/training/protocol_model.py`)
- Frozen OpenCLIP ViT-B/32 backbone
- Trainable image and text projection heads (linear or MLP)
- Optional attribute classification heads (material, pattern, neckline, sleeve)
- Optional learnable fusion weight
- CPU-friendly with minimal parameters (~1M trainable)
- Checkpoint save/load with optimizer state

**Key Features:**
- `forward_image()` / `forward_text()`: Training-time forward passes
- `encode_image_numpy()` / `encode_text_numpy()`: Inference-time encoding
- Compatible with Step 6 catalog building pipeline

#### **Training Losses** (`src/training/losses.py`)
- **InfoNCE Loss**: Contrastive retrieval objective with in-batch negatives
- **Attribute Loss**: Multi-attribute classification (material/pattern/neckline/sleeve)
  - **Neckline weighted 2x** to address Step 6 weakness
- **Combined Loss**: Weighted combination of contrastive + attribute objectives

#### **Balanced Sampler** (`src/training/sampler.py`)
- Oversamples weak categories: `cardigans`, `shorts`
- Oversamples rare materials: `denim`, `leather`
- Oversamples neckline-known samples (2x boost)
- Caps dominant groups: `cotton`, `tees`
- Per-batch distribution logging

---

### 2. Data Supervision & Pair Generation

#### **Graphic Policy** (`src/datasets/graphic_policy.py`)
- Treats `category2='graphic'` as **pattern-only** signal
- Never uses `graphic` for category-based positive/negative matching
- Filters `graphic` items from category metrics
- Integrated into pair generation

#### **Group Key Derivation** (`src/datasets/group_key.py`)
- Derives product group keys using heuristics:
  - Item ID suffix removal (`_view1`, `_A`, `_front`)
  - URL stem extraction
  - SKU pattern matching in text
- Confidence scoring for group reliability
- Enables strong positives when multi-view data available

#### **Pair Generator Updates** (`src/datasets/pair_generator.py`)
- Extended with graphic policy integration
- Filters `graphic` items from category-based matching
- Validates pairs according to Step 7 rules

---

### 3. Job Scripts (Runnable Stages)

#### **Data Quality Gate**

**`scripts/step7_grounding_audit.py`**
- Samples 200 items stratified by category
- QA CSV for manual review: "does this attribute refer to category2?"
- Coverage summary by category
- Instructions for reviewers

**`scripts/step7_analyze_grounding.py`**
- Analyzes completed grounding audit CSV
- Reports per-attribute correctness
- Identifies contamination sources
- Provides schema tuning recommendations

#### **Pre-Training Jobs**

**`scripts/step7_build_groups.py`**
- Derives product groups from dataset
- Audits group quality (spot-check sample)
- Outputs group index JSON
- Recommendations on whether to use group-based positives

**`scripts/step7_mine_hard_negatives.py`**
- Uses Step 6 baseline retrieval to mine hard negatives
- Finds "confusers" that match category but differ on attributes
- Targets: material, neckline, pattern mismatches
- Outputs hard negative cache (Parquet)

**`scripts/step7_build_pair_dataset.py`**
- Integrates all pair types:
  - Strong positives (group-based + attribute-based)
  - Medium positives (category + material)
  - Weak positives (category + design tags)
  - Hard negatives (mined + attribute-targeted)
  - Easy negatives (different category)
- Applies graphic policy
- Outputs balanced pair dataset (Parquet + stats)

#### **Training & Evaluation**

**`scripts/step7_train_protocol.py`**
- Main training loop
- Loads pairs + dataset
- Balanced sampling per batch
- Combined loss (contrastive + attribute)
- Periodic checkpointing
- Training log (JSONL)

**`scripts/step7_validate_checkpoint.py`**
- Quick validation during training
- Mini catalog (5K items) + limited queries (500)
- Fast metric computation
- Delta vs baseline

**`scripts/step7_eval_full.py`**
- Full catalog rebuild with checkpoint
- Step 6-compatible evaluation (reuses `eval_retrieval.py`)
- Scoreboard generation with deltas
- Weakness-focused gallery
- Final evaluation report

#### **Utilities**

**`scripts/step7_compute_scoreboard.py`**
- Standalone scoreboard computation
- Compares checkpoint vs baseline metrics
- Success criteria evaluation

---

### 4. Evaluation & Metrics

#### **Step 7 Scoreboard** (`src/utils/scoreboard.py`)
- Locked success targets:
  - **Neckline@10**: +5 to +10 points
  - **Category@10** for cardigans/shorts: +10 points
  - **Material@10** for denim/leather: +10 points
- Computes deltas vs Step 6 baseline
- Formatted text + JSON output
- Clear pass/fail indicators

---

## ğŸ¯ End-to-End Workflow

### Stage 0: Prerequisites (Completed in Step 6)
```bash
# Ensure Step 6 baseline exists
artifacts/retrieval/openclip_vitb32_v0/
â”œâ”€â”€ catalog_img.npy
â”œâ”€â”€ catalog_txt.npy
â”œâ”€â”€ catalog_meta.parquet
â”œâ”€â”€ manifest.json
â””â”€â”€ eval/
    â”œâ”€â”€ metrics.json
    â”œâ”€â”€ metrics_by_category.json
    â””â”€â”€ metrics_by_material.json
```

### Stage 1: Data Quality Gate (Optional but Recommended)

```bash
# 1a. Grounding audit
python scripts/step7_grounding_audit.py \
    --dataset data/processed_v2/hf \
    --split train \
    --output step7_grounding_audit.csv \
    --n_samples 200

# Manual review of CSV

# 1b. Analyze audit results
python scripts/step7_analyze_grounding.py \
    --input step7_grounding_audit_completed.csv

# If issues found, update src/datasets/attribute_schema.yaml
```

### Stage 2: Build Training Supervision

```bash
# 2a. Derive product groups (optional for strong positives)
python scripts/step7_build_groups.py \
    --dataset data/processed_v2/hf \
    --split train \
    --output artifacts/step7/groups.json \
    --min_confidence 0.6

# 2b. Mine hard negatives using Step 6 baseline
python scripts/step7_mine_hard_negatives.py \
    --catalog_dir artifacts/retrieval/openclip_vitb32_v0 \
    --dataset data/processed_v2/hf \
    --split train \
    --output artifacts/step7/hard_negatives.parquet \
    --max_anchors 10000

# 2c. Build pair dataset (integrates everything)
python scripts/step7_build_pair_dataset.py \
    --dataset data/processed_v2/hf \
    --split train \
    --groups artifacts/step7/groups.json \
    --hard_negatives artifacts/step7/hard_negatives.parquet \
    --output artifacts/step7/pairs.parquet \
    --n_pairs 50000
```

### Stage 3: Train Protocol Model

```bash
python scripts/step7_train_protocol.py \
    --pairs artifacts/step7/pairs.parquet \
    --dataset data/processed_v2/hf \
    --output artifacts/step7/runs/run_001 \
    --epochs 10 \
    --batch_size 64 \
    --lr 1e-4 \
    --seed 42
```

**Output:**
- `artifacts/step7/runs/run_001/config.json`
- `artifacts/step7/runs/run_001/train_log.jsonl`
- `artifacts/step7/runs/run_001/checkpoints/epoch_*.pt`
- `artifacts/step7/runs/run_001/checkpoints/final.pt`

### Stage 4: Validate Checkpoints (During/After Training)

```bash
# Quick validation of specific checkpoint
python scripts/step7_validate_checkpoint.py \
    --checkpoint artifacts/step7/runs/run_001/checkpoints/epoch_5.pt \
    --catalog_dir artifacts/retrieval/openclip_vitb32_v0 \
    --query_dataset data/processed_v2/hf \
    --output artifacts/step7/runs/run_001/validation/epoch_5
```

### Stage 5: Full Evaluation of Best Checkpoint

```bash
python scripts/step7_eval_full.py \
    --checkpoint artifacts/step7/runs/run_001/checkpoints/best.pt \
    --dataset data/processed_v2/hf \
    --baseline_metrics artifacts/retrieval/openclip_vitb32_v0/eval/metrics.json \
    --output artifacts/step7/runs/run_001/eval_full
```

**Output:**
- Full catalog with checkpoint embeddings
- Step 6-compatible metrics
- Scoreboard with deltas vs baseline
- Weakness-focused gallery
- Final evaluation report

---

## ğŸ“Š Success Criteria (Step 7 Scoreboard)

The protocol model is considered successful if:

| Metric | Target | Why It Matters |
|--------|--------|----------------|
| **Neckline@10** | +5 to +10 pts | Address Step 6's weakest attribute (18.3% baseline) |
| **Cardigans Category@10** | +10 pts | Fix weak class performance |
| **Shorts Category@10** | +10 pts | Fix weak class performance |
| **Denim Material@10** | +10 pts | Improve rare material retrieval |
| **Leather Material@10** | +10 pts | Improve rare material retrieval |

**Overall Success:** Neckline improves by +5pt AND all weak classes/rare materials show positive gains.

---

## ğŸ”§ Configuration & Tuning

### Key Hyperparameters

**Training:**
- Batch size: 64 (CPU-friendly)
- Learning rate: 1e-4 (AdamW with weight decay 0.01)
- Epochs: 10 (short experiments)
- Contrastive loss weight: 1.0
- Attribute loss weight: 0.3
- Neckline loss weight: 2.0 (extra emphasis)

**Sampling:**
- Weak category boost: 3.0x
- Rare material boost: 3.0x
- Neckline-known boost: 2.0x
- Dominant cap: 30% max per batch

**Pair Distribution:**
- Strong positives: 25-35%
- Medium positives: 20%
- Weak positives: 10%
- Hard negatives: 25-30%
- Easy negatives: 5%

### If Metrics Don't Improve

**Neckline still low?**
- Increase neckline loss weight (2.0 â†’ 4.0)
- Increase neckline-known sampling boost
- Review grounding audit for neckline contamination

**Weak classes still weak?**
- Increase weak category boost (3.0 â†’ 5.0)
- Mine more hard negatives specifically for these categories
- Check if dataset has sufficient samples

**Overfitting?**
- Reduce epochs
- Increase weight decay
- Add more dropout in projection heads

---

## ğŸ‰ What's Different from Step 6?

| Aspect | Step 6 (Baseline) | Step 7 (Protocol) |
|--------|-------------------|-------------------|
| **Model** | Frozen OpenCLIP (zero-shot) | Frozen backbone + trained adapters |
| **Training Data** | None | 50K attribute-aware pairs |
| **Hard Negatives** | Random in-batch | Mined confusers from Step 6 |
| **Attribute Emphasis** | None | Explicit neckline/material objectives |
| **Sampling** | Uniform | Balanced (weak classes + rare materials) |
| **Target** | General retrieval | Attribute-aware retrieval |

---

## ğŸ“ File Structure Created

```
src/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ graphic_policy.py       # Graphic handling
â”‚   â”œâ”€â”€ group_key.py             # Product group derivation
â”‚   â””â”€â”€ pair_generator.py        # Updated with graphic policy
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ protocol_model.py        # Main model
â”‚   â”œâ”€â”€ losses.py                # Training objectives
â”‚   â””â”€â”€ sampler.py               # Balanced sampling
â””â”€â”€ utils/
    â””â”€â”€ scoreboard.py            # Step 7 metrics

scripts/
â”œâ”€â”€ step7_grounding_audit.py     # QA job
â”œâ”€â”€ step7_analyze_grounding.py   # Audit analysis
â”œâ”€â”€ step7_build_groups.py        # Group derivation
â”œâ”€â”€ step7_mine_hard_negatives.py # Hard negative mining
â”œâ”€â”€ step7_build_pair_dataset.py  # Pair dataset builder
â”œâ”€â”€ step7_train_protocol.py      # Training entrypoint
â”œâ”€â”€ step7_validate_checkpoint.py # Quick validation
â”œâ”€â”€ step7_eval_full.py           # Full evaluation
â””â”€â”€ step7_compute_scoreboard.py  # Standalone scoreboard

artifacts/step7/                 # Created during execution
â”œâ”€â”€ groups.json
â”œâ”€â”€ hard_negatives.parquet
â”œâ”€â”€ pairs.parquet
â”œâ”€â”€ pairs_stats.json
â””â”€â”€ runs/
    â””â”€â”€ run_001/
        â”œâ”€â”€ config.json
        â”œâ”€â”€ train_log.jsonl
        â”œâ”€â”€ checkpoints/
        â”œâ”€â”€ validation/
        â””â”€â”€ eval_full/
```

---

## âœ… Implementation Status: **COMPLETE**

All Step 7 components are implemented and ready for use. The system provides:

1. âœ… CPU-friendly fine-tuning with frozen backbone
2. âœ… Attribute-aware supervision (neckline emphasis)
3. âœ… Balanced sampling (weak classes + rare materials)
4. âœ… Mined hard negatives from Step 6
5. âœ… Graphic policy to reduce taxonomy noise
6. âœ… Group-based strong positives (when available)
7. âœ… Step 6-compatible evaluation + galleries
8. âœ… Clear scoreboard with success criteria

**Next:** Run the workflow and iterate based on scoreboard results!

