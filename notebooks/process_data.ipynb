{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6365ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52a0b9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEEPFASHION DATA AUDIT & CLEANING ===\n",
      "\n",
      "1. Loading dataset...\n",
      "   Dataset loaded with 38283 examples\n",
      "\n",
      "2. Auditing images and basic fields...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auditing samples: 100%|██████████| 38283/38283 [00:29<00:00, 1303.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Generating audit report...\n",
      "\n",
      "=== DATA AUDIT REPORT ===\n",
      "Total samples: 38283\n",
      "────────────────────────────────────────\n",
      "\n",
      "IMAGE AUDIT:\n",
      "• Corrupted images: 0 (0.0%)\n",
      "• Empty images: 0 (0.0%)\n",
      "• Tiny images (<32x32): 0 (0.0%)\n",
      "\n",
      "TEXT AUDIT:\n",
      "• Missing text: 0 (0.0%)\n",
      "• Empty text: 0 (0.0%)\n",
      "• Average text length: 225.8 characters\n",
      "• Min text length: 23\n",
      "• Max text length: 514\n",
      "\n",
      "CATEGORY3 AUDIT:\n",
      "• None values: 38283 (100.0%)\n",
      "• Unique non-null values: 0\n",
      "\n",
      "TEXT QUALITY ANALYSIS:\n",
      "• Samples with fabric mentions: 33687 (88.0%)\n",
      "• Samples with design attributes: 38273 (100.0%)\n",
      "\n",
      "CLEANING SUMMARY:\n",
      "• Total samples to remove: 0\n",
      "• Clean samples remaining: 38283\n",
      "\n",
      "\n",
      "TOP 10 CATEGORIES (category2):\n",
      "  tees: 11982 (31.3%)\n",
      "  blouses: 6742 (17.6%)\n",
      "  dresses: 6001 (15.7%)\n",
      "  sweaters: 2980 (7.8%)\n",
      "  jackets: 1949 (5.1%)\n",
      "  rompers: 1477 (3.9%)\n",
      "  shorts: 1356 (3.5%)\n",
      "  sweatshirts: 1267 (3.3%)\n",
      "  cardigans: 1258 (3.3%)\n",
      "  graphic: 1016 (2.7%)\n",
      "\n",
      "CATEGORY3 VALUES:\n",
      "  None: 38283 (100.0%)\n",
      "\n",
      "4. Creating cleaned dataset...\n",
      "   Removing category3 column (mostly None)...\n",
      "   Cleaned dataset: 38283 samples\n",
      "\n",
      "5. Saving cleaned dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 38283/38283 [00:00<00:00, 80123.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Saved to: ../data/processed/cleaned_hf\n",
      "\n",
      "✅ Data audit and cleaning completed!\n",
      "   Original: 38283 samples\n",
      "   Cleaned:  38283 samples\n",
      "   Removed:  0 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add project root to path\n",
    "notebook_dir = Path.cwd()\n",
    "if notebook_dir.name == 'notebooks':\n",
    "    project_root = notebook_dir.parent\n",
    "else:\n",
    "    project_root = notebook_dir\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "print(\"=== DEEPFASHION DATA AUDIT & CLEANING ===\\n\")\n",
    "\n",
    "# Load dataset\n",
    "print(\"1. Loading dataset...\")\n",
    "ds = load_from_disk(\"../data/processed/hf\")\n",
    "train_ds = ds[\"train\"]\n",
    "\n",
    "print(f\"   Dataset loaded with {len(train_ds)} examples\")\n",
    "\n",
    "# Initialize audit counters\n",
    "audit_results = {\n",
    "    'total_samples': len(train_ds),\n",
    "    'corrupted_images': 0,\n",
    "    'empty_images': 0,\n",
    "    'tiny_images': 0,\n",
    "    'missing_text': 0,\n",
    "    'empty_text': 0,\n",
    "    'category3_none_count': 0,\n",
    "    'fabric_mentions': 0,\n",
    "    'design_attributes': 0,\n",
    "    'text_lengths': [],\n",
    "    'image_sizes': [],\n",
    "    'category1_distribution': Counter(),\n",
    "    'category2_distribution': Counter(),\n",
    "    'category3_values': Counter(),\n",
    "    'samples_to_remove': []\n",
    "}\n",
    "\n",
    "print(\"\\n2. Auditing images and basic fields...\")\n",
    "\n",
    "# Audit each sample\n",
    "for idx, example in enumerate(tqdm(train_ds, desc=\"Auditing samples\")):\n",
    "    try:\n",
    "        # Check image\n",
    "        image = example['image']\n",
    "        if image is None:\n",
    "            audit_results['samples_to_remove'].append(idx)\n",
    "            audit_results['empty_images'] += 1\n",
    "            continue\n",
    "            \n",
    "        # Check image size (PIL images have size attribute)\n",
    "        width, height = image.size\n",
    "        audit_results['image_sizes'].append((width, height))\n",
    "        \n",
    "        # Flag tiny images (< 32x32 pixels)\n",
    "        if width < 32 or height < 32:\n",
    "            audit_results['samples_to_remove'].append(idx)\n",
    "            audit_results['tiny_images'] += 1\n",
    "            continue\n",
    "            \n",
    "        # Check text\n",
    "        text = example.get('text', '')\n",
    "        if text is None:\n",
    "            audit_results['missing_text'] += 1\n",
    "            audit_results['samples_to_remove'].append(idx)\n",
    "            continue\n",
    "            \n",
    "        text = str(text).strip()\n",
    "        if len(text) == 0:\n",
    "            audit_results['empty_text'] += 1\n",
    "            audit_results['samples_to_remove'].append(idx)\n",
    "            continue\n",
    "            \n",
    "        audit_results['text_lengths'].append(len(text))\n",
    "        \n",
    "        # Check category3\n",
    "        category3 = example.get('category3')\n",
    "        if category3 is None:\n",
    "            audit_results['category3_none_count'] += 1\n",
    "        audit_results['category3_values'][str(category3)] += 1\n",
    "        \n",
    "        # Count distributions\n",
    "        audit_results['category1_distribution'][example.get('category1', 'unknown')] += 1\n",
    "        audit_results['category2_distribution'][example.get('category2', 'unknown')] += 1\n",
    "        \n",
    "        # Text quality analysis\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check for fabric mentions\n",
    "        fabric_keywords = ['cotton', 'silk', 'wool', 'linen', 'polyester', 'nylon', 'spandex', 'rayon', 'denim', 'leather', 'suede']\n",
    "        if any(fabric in text_lower for fabric in fabric_keywords):\n",
    "            audit_results['fabric_mentions'] += 1\n",
    "            \n",
    "        # Check for design attributes\n",
    "        design_keywords = ['pattern', 'striped', 'checked', 'printed', 'solid', 'neckline', 'sleeve', 'collar', 'crew', 'v-neck', 'hood']\n",
    "        if any(design in text_lower for design in design_keywords):\n",
    "            audit_results['design_attributes'] += 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Error processing sample {idx}: {e}\")\n",
    "        audit_results['corrupted_images'] += 1\n",
    "        audit_results['samples_to_remove'].append(idx)\n",
    "\n",
    "print(\"\\n3. Generating audit report...\")\n",
    "\n",
    "# Calculate statistics\n",
    "total_samples = audit_results['total_samples']\n",
    "image_sizes = audit_results['image_sizes']\n",
    "text_lengths = audit_results['text_lengths']\n",
    "\n",
    "print(f\"\"\"\n",
    "=== DATA AUDIT REPORT ===\n",
    "Total samples: {total_samples}\n",
    "{'─' * 40}\n",
    "\n",
    "IMAGE AUDIT:\n",
    "• Corrupted images: {audit_results['corrupted_images']} ({audit_results['corrupted_images']/total_samples*100:.1f}%)\n",
    "• Empty images: {audit_results['empty_images']} ({audit_results['empty_images']/total_samples*100:.1f}%)\n",
    "• Tiny images (<32x32): {audit_results['tiny_images']} ({audit_results['tiny_images']/total_samples*100:.1f}%)\n",
    "\n",
    "TEXT AUDIT:\n",
    "• Missing text: {audit_results['missing_text']} ({audit_results['missing_text']/total_samples*100:.1f}%)\n",
    "• Empty text: {audit_results['empty_text']} ({audit_results['empty_text']/total_samples*100:.1f}%)\n",
    "• Average text length: {sum(text_lengths)/len(text_lengths):.1f} characters\n",
    "• Min text length: {min(text_lengths) if text_lengths else 0}\n",
    "• Max text length: {max(text_lengths) if text_lengths else 0}\n",
    "\n",
    "CATEGORY3 AUDIT:\n",
    "• None values: {audit_results['category3_none_count']} ({audit_results['category3_none_count']/total_samples*100:.1f}%)\n",
    "• Unique non-null values: {len([k for k in audit_results['category3_values'].keys() if k != 'None'])}\n",
    "\n",
    "TEXT QUALITY ANALYSIS:\n",
    "• Samples with fabric mentions: {audit_results['fabric_mentions']} ({audit_results['fabric_mentions']/total_samples*100:.1f}%)\n",
    "• Samples with design attributes: {audit_results['design_attributes']} ({audit_results['design_attributes']/total_samples*100:.1f}%)\n",
    "\n",
    "CLEANING SUMMARY:\n",
    "• Total samples to remove: {len(set(audit_results['samples_to_remove']))}\n",
    "• Clean samples remaining: {total_samples - len(set(audit_results['samples_to_remove']))}\n",
    "\"\"\")\n",
    "\n",
    "# Show top categories\n",
    "print(\"\\nTOP 10 CATEGORIES (category2):\")\n",
    "for cat, count in audit_results['category2_distribution'].most_common(10):\n",
    "    print(f\"  {cat}: {count} ({count/total_samples*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nCATEGORY3 VALUES:\")\n",
    "for val, count in audit_results['category3_values'].most_common():\n",
    "    print(f\"  {val}: {count} ({count/total_samples*100:.1f}%)\")\n",
    "# Create cleaned dataset\n",
    "print(\"\\n4. Creating cleaned dataset...\")\n",
    "clean_indices = [i for i in range(total_samples) if i not in set(audit_results['samples_to_remove'])]\n",
    "cleaned_train_ds = train_ds.select(clean_indices)\n",
    "\n",
    "# Remove category3 column since it's mostly None\n",
    "if audit_results['category3_none_count'] / total_samples > 0.95:  # If >95% None\n",
    "    print(\"   Removing category3 column (mostly None)...\")\n",
    "    cleaned_train_ds = cleaned_train_ds.remove_columns(['category3'])\n",
    "\n",
    "print(f\"   Cleaned dataset: {len(cleaned_train_ds)} samples\")\n",
    "\n",
    "# Save cleaned dataset\n",
    "print(\"\\n5. Saving cleaned dataset...\")\n",
    "output_path = \"../data/processed/cleaned_hf\"\n",
    "cleaned_train_ds.save_to_disk(output_path)\n",
    "print(f\"   Saved to: {output_path}\")\n",
    "\n",
    "print(\"\\n✅ Data audit and cleaning completed!\")\n",
    "print(f\"   Original: {total_samples} samples\")\n",
    "print(f\"   Cleaned:  {len(cleaned_train_ds)} samples\")\n",
    "print(f\"   Removed:  {len(set(audit_results['samples_to_remove']))} samples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
