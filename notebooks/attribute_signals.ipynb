{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion Attribute Signal Extraction\n",
    "\n",
    "This notebook implements Step 5 from the project checklist: \"Build structured attribute signals from text (materials + design cues)\"\n",
    "\n",
    "The goal is to extract structured fashion attributes from product text descriptions to enable better similarity matching for garment design and materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from typing import List, Dict, Set\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset and Define Attribute Schema\n",
    "\n",
    "Load the cleaned dataset and define the attribute extraction schema based on the project requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned dataset...\n",
      "Dataset loaded with 38283 examples\n",
      "Features: {'image': Image(mode=None, decode=True), 'category1': Value('string'), 'category2': Value('string'), 'category3': Value('float64'), 'text': Value('string'), 'item_ID': Value('string')}\n",
      "Attribute schema defined with categories: ['material', 'pattern', 'neckline', 'sleeve', 'closure']\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned dataset\n",
    "print(\"Loading cleaned dataset...\")\n",
    "ds = load_from_disk(\"../data/processed/hf\")\n",
    "train_ds = ds[\"train\"]\n",
    "print(f\"Dataset loaded with {len(train_ds)} examples\")\n",
    "print(f\"Features: {train_ds.features}\")\n",
    "\n",
    "# Define attribute schema based on project requirements\n",
    "ATTRIBUTE_SCHEMA = {\n",
    "    \"material\": {\n",
    "        \"keywords\": [\n",
    "            \"cotton\", \"denim\", \"knit\", \"leather\", \"polyester\", \"silk\", \"wool\", \n",
    "            \"linen\", \"nylon\", \"spandex\", \"rayon\", \"acrylic\", \"velvet\", \"satin\",\n",
    "            \"chiffon\", \"lace\", \"mesh\", \"jersey\", \"fleece\", \"tulle\", \"organza\"\n",
    "        ],\n",
    "        \"description\": \"Fabric type and material composition\"\n",
    "    },\n",
    "    \"pattern\": {\n",
    "    \"keywords\": [\n",
    "        # Original\n",
    "        \"solid\", \"stripe\", \"plaid\", \"check\", \"polka dot\", \"floral\", \"graphic\",\n",
    "        \"animal print\", \"camouflage\", \"color-block\", \"ombre\", \"tie-dye\",\n",
    "        \"paisley\", \"houndstooth\", \"gingham\", \"argyle\", \"chevron\",\n",
    "        # Missing terms\n",
    "        \"pure color\", \"plain\", \"solid color\", \"single color\", \"monochrome\",\n",
    "        \"geometric\", \"abstract\", \"motif\", \"print\", \"design\", \"patterned\"\n",
    "    ]\n",
    "},\n",
    "    \"neckline\": {\n",
    "    \"keywords\": [\n",
    "        # Basic styles\n",
    "        \"crew\", \"v-neck\", \"scoop\", \"boat\", \"square\", \"halter\", \"sweetheart\",\n",
    "        \"off-the-shoulder\", \"turtleneck\", \"mock neck\", \"collar\", \"peter pan\",\n",
    "        \"cowl\", \"asymmetric\", \"keyhole\", \"round\", \"stand\", \"lapel\", \"v-shape\",\n",
    "        # Compound terms  \n",
    "        \"crew neck\", \"v neck\", \"scoop neck\", \"boat neck\", \"square neck\", \n",
    "        \"sweetheart neck\", \"halter neck\", \"turtle neck\", \"mockneck\", \n",
    "        \"mandarin collar\", \"notched collar\", \"shirt collar\", \"lapel collar\",\n",
    "        \"stand collar\", \"round neck\", \"v-shape neck\"\n",
    "    ]\n",
    "},\n",
    "    \"sleeve\": {\n",
    "        \"keywords\": [\n",
    "            \"sleeveless\", \"short\", \"long\", \"3/4\", \"cap\", \"bell\", \"puff\",\n",
    "            \"raglan\", \"bishop\", \"flutter\", \"batwing\", \"dolman\", \"cold shoulder\"\n",
    "        ],\n",
    "        \"description\": \"Sleeve length and style\"\n",
    "    },\n",
    "    \"closure\": {\n",
    "        \"keywords\": [\n",
    "            \"button\", \"zipper\", \"snap\", \"hook\", \"tie\", \"velcro\", \"magnetic\",\n",
    "            \"elastic\", \"drawstring\", \"frog\", \"buckle\", \"toggle\"\n",
    "        ],\n",
    "        \"description\": \"Fastening and closure mechanisms\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Attribute schema defined with categories:\", list(ATTRIBUTE_SCHEMA.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Attribute Extraction Functions\n",
    "\n",
    "Implement rule-based attribute extraction functions that search for keywords in the text descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: This woman wears a short-sleeve T-shirt with color block patterns and a long pants. The T-shirt is with cotton fabric and its neckline is crew. The pants are with cotton fabric and solid color patterns. This female is wearing a ring on her finger. There is an accessory on her wrist.\n",
      "Target category: blouses\n",
      "Extracted attributes:\n",
      "  material: ['cotton'] (primary: cotton)\n",
      "  pattern: [] (primary: )\n",
      "  neckline: ['crew'] (primary: crew)\n",
      "  sleeve: ['long', 'short'] (primary: long)\n",
      "  closure: [] (primary: )\n"
     ]
    }
   ],
   "source": [
    "def extract_with_target_anchoring(text: str, target_category: str, schema: Dict) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract attributes by anchoring to the target garment throughout the text.\n",
    "    \"\"\"\n",
    "    target_scope = get_garment_scope(target_category)\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Define anchor patterns for each garment type\n",
    "    anchor_patterns = {\n",
    "        'upper': [\n",
    "            r'\\b(?:the\\s+)?(?:shirt|blouse|top|jacket|coat|sweater|cardigan|vest|t-shirt)\\b',\n",
    "            r'\\b(?:her|his|their|the)\\s+(?:shirt|blouse|top|jacket|coat|sweater)\\b'\n",
    "        ],\n",
    "        'lower': [\n",
    "            r'\\b(?:the\\s+)?(?:pants|jeans|shorts|skirt|leggings|trousers)\\b', \n",
    "            r'\\b(?:her|his|their|the)\\s+(?:pants|jeans|shorts|skirt|leggings)\\b'\n",
    "        ],\n",
    "        'full': [\n",
    "            r'\\b(?:the\\s+)?(?:dress|jumpsuit|romper|overall|gown)\\b',\n",
    "            r'\\b(?:her|his|their|the)\\s+(?:dress|jumpsuit|romper|overall)\\b'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    attributes = {}\n",
    "    for attr_name, attr_config in schema.items():\n",
    "        found_keywords = []\n",
    "        \n",
    "        for keyword in attr_config[\"keywords\"]:\n",
    "            keyword_lower = keyword.lower()\n",
    "            \n",
    "            # Check if keyword appears near target garment anchors\n",
    "            if is_keyword_anchored_to_target(keyword_lower, text_lower, anchor_patterns[target_scope]):\n",
    "                if keyword not in found_keywords:  # Avoid duplicates\n",
    "                    found_keywords.append(keyword)\n",
    "        \n",
    "        attributes[attr_name] = found_keywords\n",
    "    \n",
    "    return attributes\n",
    "\n",
    "def is_keyword_anchored_to_target(keyword: str, text: str, anchor_patterns: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a keyword is syntactically linked to the target garment.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Find all anchor positions\n",
    "    anchor_positions = []\n",
    "    for pattern in anchor_patterns:\n",
    "        for match in re.finditer(pattern, text):\n",
    "            anchor_positions.append(match.start())\n",
    "    \n",
    "    if not anchor_positions:\n",
    "        return False\n",
    "    \n",
    "    # Find keyword positions\n",
    "    keyword_positions = []\n",
    "    start = 0\n",
    "    while True:\n",
    "        pos = text.find(keyword, start)\n",
    "        if pos == -1:\n",
    "            break\n",
    "        keyword_positions.append(pos)\n",
    "        start = pos + 1\n",
    "    \n",
    "    # Check if any keyword is within reasonable distance of any anchor\n",
    "    max_distance = 100  # characters\n",
    "    \n",
    "    for kw_pos in keyword_positions:\n",
    "        for anchor_pos in anchor_positions:\n",
    "            if abs(kw_pos - anchor_pos) <= max_distance:\n",
    "                # Additional check: no competing garment mentions in between\n",
    "                between_text = text[min(kw_pos, anchor_pos):max(kw_pos, anchor_pos)]\n",
    "                competing_garments = ['pants', 'jeans', 'shirt', 'blouse', 'skirt', 'dress']\n",
    "                has_competition = any(garment in between_text for garment in competing_garments \n",
    "                                    if garment not in anchor_patterns[0])  # Don't count target garment as competition\n",
    "                \n",
    "                if not has_competition:\n",
    "                    return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def extract_garment_specific_attributes(text: str, target_category: str, schema: Dict) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract attributes specific to the target garment type.\n",
    "    \"\"\"\n",
    "    # Use the existing get_garment_scope function\n",
    "    target_scope = get_garment_scope(target_category)\n",
    "    \n",
    "    # Split text into garment-specific segments\n",
    "    segments = segment_text_by_garments(text, target_scope)\n",
    "    \n",
    "    # Extract only from relevant segments\n",
    "    attributes = {}\n",
    "    for attr_name, attr_config in schema.items():\n",
    "        found_keywords = []\n",
    "        for segment in segments:\n",
    "            for keyword in attr_config[\"keywords\"]:\n",
    "                if is_attribute_linked_to_garment(keyword, segment, target_scope):\n",
    "                    found_keywords.append(keyword)\n",
    "        \n",
    "        attributes[attr_name] = list(set(found_keywords))  # Remove duplicates\n",
    "    \n",
    "    return attributes\n",
    "\n",
    "def segment_text_by_garments(text: str, target_scope: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Improved segmentation that keeps more context while avoiding contamination.\n",
    "    \"\"\"\n",
    "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    \n",
    "    if target_scope == 'full':\n",
    "        return sentences\n",
    "    \n",
    "    relevant_segments = []\n",
    "    garment_indicators = {\n",
    "        'upper': ['shirt', 'blouse', 'top', 'jacket', 'coat', 'sweater', 't-shirt', 'cardigan', 'vest'],\n",
    "        'lower': ['pants', 'jeans', 'shorts', 'skirt', 'leggings', 'trousers', 'legging']\n",
    "    }\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_lower = sentence.lower()\n",
    "        \n",
    "        # Include sentences that mention target garment\n",
    "        if any(indicator in sentence_lower for indicator in garment_indicators[target_scope]):\n",
    "            relevant_segments.append(sentence)\n",
    "        # For upper garments, include general description sentences that don't mention lower garments\n",
    "        elif target_scope == 'upper' and not any(lower_ind in sentence_lower for lower_ind in garment_indicators['lower']):\n",
    "            # Additional check: look for upper garment attributes\n",
    "            upper_attrs = ['sleeve', 'neck', 'collar', 'button', 'zipper']\n",
    "            if any(attr in sentence_lower for attr in upper_attrs):\n",
    "                relevant_segments.append(sentence)\n",
    "    \n",
    "    return relevant_segments if relevant_segments else sentences\n",
    "\n",
    "\n",
    "def is_attribute_linked_to_garment(keyword: str, segment: str, target_scope: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if an attribute keyword is syntactically linked to the target garment.\n",
    "    \"\"\"\n",
    "    segment_lower = segment.lower()\n",
    "    keyword_lower = keyword.lower()\n",
    "    \n",
    "    # Find garment mentions in the segment\n",
    "    garment_indicators = {\n",
    "    'upper': ['shirt', 'blouse', 'top', 'jacket', 'coat', 'sweater', 't-shirt', 'cardigan'],\n",
    "    'lower': ['pants', 'jeans', 'shorts', 'skirt', 'leggings', 'trousers'],\n",
    "    'full': ['dress', 'jumpsuit', 'romper', 'overall']\n",
    "    }\n",
    "    \n",
    "    # Look for explicit attribute-garment relationships\n",
    "    # Pattern: \"garment is/with/has [attribute]\"\n",
    "    for indicator in garment_indicators[target_scope]:\n",
    "        if indicator in segment_lower:\n",
    "            # Check for connecting words\n",
    "            patterns = [\n",
    "                f\"{indicator}.*?{keyword_lower}\",\n",
    "                f\"{keyword_lower}.*?{indicator}\",\n",
    "                f\"{indicator}.*?(is|with|has).*?{keyword_lower}\",\n",
    "                f\"{keyword_lower}.*?(is|with|has).*?{indicator}\"\n",
    "            ]\n",
    "            for pattern in patterns:\n",
    "                if re.search(pattern, segment_lower, re.IGNORECASE):\n",
    "                    return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def get_primary_attribute_smart(attr_list: List[str], text: str, attr_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Select primary attribute with text-position priority, not semantic interpretation.\n",
    "    \"\"\"\n",
    "    if not attr_list:\n",
    "        return \"\"\n",
    "    \n",
    "    if len(attr_list) == 1:\n",
    "        return attr_list[0]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Priority: First occurrence in text, not semantic similarity\n",
    "    for attr in attr_list:\n",
    "        if attr.lower() in text_lower:\n",
    "            # Find the earliest position of this attribute in text\n",
    "            attr_pos = text_lower.find(attr.lower())\n",
    "            if attr_pos != -1:\n",
    "                return attr\n",
    "    \n",
    "    # Fallback to first in list\n",
    "    return attr_list[0]\n",
    "\n",
    "def apply_negative_constraints(attributes: Dict[str, List[str]], target_scope: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Apply negative constraints to prevent impossible attribute combinations.\n",
    "    \"\"\"\n",
    "    filtered = attributes.copy()\n",
    "    \n",
    "    # Sleeve constraints for upper garments\n",
    "    if target_scope == 'upper':\n",
    "        sleeve_attrs = filtered.get('sleeve', [])\n",
    "        # Prevent impossible combinations like sleeveless + long\n",
    "        if 'sleeveless' in sleeve_attrs and any(long in sleeve_attrs for long in ['long', '3/4']):\n",
    "            # Keep sleeveless, remove conflicting attributes\n",
    "            filtered['sleeve'] = ['sleeveless']\n",
    "    \n",
    "    # Add more constraints as needed...\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "\n",
    "\n",
    "def get_primary_attribute(attr_list: List[str], priority_order: List[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Select primary attribute from list (first match or based on priority).\n",
    "    \n",
    "    Args:\n",
    "        attr_list: List of found attributes\n",
    "        priority_order: Optional priority ordering\n",
    "    \n",
    "    Returns:\n",
    "        Primary attribute string or empty string\n",
    "    \"\"\"\n",
    "    if not attr_list:\n",
    "        return \"\"\n",
    "    \n",
    "    if priority_order:\n",
    "        for priority in priority_order:\n",
    "            if priority in attr_list:\n",
    "                return priority\n",
    "    \n",
    "    return attr_list[0]\n",
    "\n",
    "def get_garment_scope(target_category: str) -> str:\n",
    "    \"\"\"\n",
    "    Determine the garment scope based on category2 value.\n",
    "    \n",
    "    Args:\n",
    "        target_category: The category2 value from the dataset (e.g., 'blouses')\n",
    "    \n",
    "    Returns:\n",
    "        Garment scope: 'upper', 'lower', or 'full'\n",
    "    \"\"\"\n",
    "    # Map category2 values to garment scopes\n",
    "    GARMENT_TYPES = {\n",
    "        'upper': [\n",
    "            'blouses', 'shirts', 't-shirts', 'tops', 'jackets', 'coats', \n",
    "            'sweaters', 'cardigans', 'blazers', 'vests', 'hoodies'\n",
    "        ],\n",
    "        'lower': [\n",
    "            'pants', 'jeans', 'shorts', 'skirts', 'leggings', 'trousers',\n",
    "            'capris', 'culottes'\n",
    "        ],\n",
    "        'full': [\n",
    "            'dresses', 'jumpsuits', 'rompers', 'overalls', 'gowns'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    target_lower = target_category.lower()\n",
    "    for scope, categories in GARMENT_TYPES.items():\n",
    "        if target_lower in categories:\n",
    "            return scope\n",
    "    \n",
    "    # Default fallback - most fashion items are upper body focused\n",
    "    return 'upper'\n",
    "\n",
    "def process_example_improved(example: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Process a single example to extract garment-specific attributes.\n",
    "    \n",
    "    Args:\n",
    "        example: Dataset example with 'text' and 'category2' fields\n",
    "    \n",
    "    Returns:\n",
    "        Example with added attribute fields\n",
    "    \"\"\"\n",
    "    text = example.get(\"text\", \"\")\n",
    "    target_category = example.get(\"category2\", \"\")\n",
    "    \n",
    "    # Use the new garment-specific extraction\n",
    "    extracted = extract_garment_specific_attributes(text, target_category, ATTRIBUTE_SCHEMA)\n",
    "    \n",
    "    # Apply negative constraints\n",
    "    extracted = apply_negative_constraints(extracted, get_garment_scope(target_category))\n",
    "    \n",
    "    # Add to example\n",
    "    for attr_name in ATTRIBUTE_SCHEMA.keys():\n",
    "        attr_key = f\"attr_{attr_name}\"\n",
    "        primary_key = f\"attr_{attr_name}_primary\"\n",
    "        \n",
    "        example[attr_key] = extracted[attr_name]\n",
    "        example[primary_key] = get_primary_attribute_smart(extracted[attr_name], text, attr_name)\n",
    "    \n",
    "    return example\n",
    "\n",
    "\n",
    "# Test the improved extraction on a sample\n",
    "sample_example = train_ds[0]\n",
    "print(\"Sample text:\", sample_example[\"text\"])\n",
    "print(\"Target category:\", sample_example[\"category2\"])\n",
    "processed_sample = process_example_improved(sample_example.copy())\n",
    "print(\"Extracted attributes:\")\n",
    "for attr_name in ATTRIBUTE_SCHEMA.keys():\n",
    "    attr_key = f\"attr_{attr_name}\"\n",
    "    primary_key = f\"attr_{attr_name}_primary\"\n",
    "    print(f\"  {attr_name}: {processed_sample[attr_key]} (primary: {processed_sample[primary_key]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Process Sample Dataset\n",
    "\n",
    "Process a small sample of the dataset to test and validate the extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample of 1000 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sample: 100%|██████████| 1000/1000 [00:39<00:00, 25.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample processing complete. Shape: (1000, 16)\n",
      "\n",
      "Attribute extraction statistics on sample:\n",
      "material: 673/1000 examples with attributes (673 with primary)\n",
      "pattern: 572/1000 examples with attributes (572 with primary)\n",
      "neckline: 147/1000 examples with attributes (147 with primary)\n",
      "sleeve: 468/1000 examples with attributes (468 with primary)\n",
      "closure: 0/1000 examples with attributes (0 with primary)\n"
     ]
    }
   ],
   "source": [
    "# Process a sample of the dataset for testing\n",
    "sample_size = min(1000, len(train_ds))\n",
    "print(f\"Processing sample of {sample_size} examples...\")\n",
    "\n",
    "# Take a sample for testing\n",
    "sample_indices = random.sample(range(len(train_ds)), sample_size)\n",
    "sample_examples = [train_ds[i] for i in sample_indices]\n",
    "\n",
    "# Process sample\n",
    "processed_sample = []\n",
    "for example in tqdm(sample_examples, desc=\"Processing sample\"):\n",
    "    processed_sample.append(process_example_improved(example.copy()))  # FIXED\n",
    "\n",
    "# Convert to Dataset for analysis\n",
    "sample_ds = Dataset.from_list(processed_sample)\n",
    "print(f\"Sample processing complete. Shape: {sample_ds.shape}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nAttribute extraction statistics on sample:\")\n",
    "for attr_name in ATTRIBUTE_SCHEMA.keys():\n",
    "    attr_key = f\"attr_{attr_name}\"\n",
    "    primary_key = f\"attr_{attr_name}_primary\"\n",
    "    \n",
    "    # Count examples with any attributes\n",
    "    has_attr = sum(1 for ex in processed_sample if ex[attr_key])\n",
    "    \n",
    "    # Count examples with primary attribute\n",
    "    has_primary = sum(1 for ex in processed_sample if ex[primary_key])\n",
    "    \n",
    "    print(f\"{attr_name}: {has_attr}/{sample_size} examples with attributes ({has_primary} with primary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Extraction Quality\n",
    "\n",
    "Analyze the quality of attribute extraction and identify common patterns or issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extraction Quality Analysis:\n",
      "==================================================\n",
      "\n",
      "MATERIAL:\n",
      "  Coverage: 673/1000 examples (67.3%)\n",
      "  Primary: 673/1000 examples (67.3%)\n",
      "  Unique attributes found: 4\n",
      "  Total mentions: 673\n",
      "  Top primary attributes: cotton(553), chiffon(75), knit(34), denim(11)\n",
      "\n",
      "PATTERN:\n",
      "  Coverage: 572/1000 examples (57.2%)\n",
      "  Primary: 572/1000 examples (57.2%)\n",
      "  Unique attributes found: 7\n",
      "  Total mentions: 758\n",
      "  Top primary attributes: solid color(180), graphic(179), pure color(155), stripe(34), floral(22)\n",
      "\n",
      "NECKLINE:\n",
      "  Coverage: 147/1000 examples (14.7%)\n",
      "  Primary: 147/1000 examples (14.7%)\n",
      "  Unique attributes found: 4\n",
      "  Total mentions: 168\n",
      "  Top primary attributes: crew(63), round(55), suspenders(29)\n",
      "\n",
      "SLEEVE:\n",
      "  Coverage: 468/1000 examples (46.8%)\n",
      "  Primary: 468/1000 examples (46.8%)\n",
      "  Unique attributes found: 3\n",
      "  Total mentions: 490\n",
      "  Top primary attributes: long(259), short(157), sleeveless(52)\n",
      "\n",
      "CLOSURE:\n",
      "  Coverage: 0/1000 examples (0.0%)\n",
      "  Primary: 0/1000 examples (0.0%)\n",
      "  Unique attributes found: 0\n",
      "  Total mentions: 0\n"
     ]
    }
   ],
   "source": [
    "# Analyze extraction quality\n",
    "def analyze_extraction_quality(processed_examples: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze the quality of attribute extraction.\n",
    "    \n",
    "    Args:\n",
    "        processed_examples: List of processed examples with attributes\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with quality metrics\n",
    "    \"\"\"\n",
    "    total_examples = len(processed_examples)\n",
    "    \n",
    "    # Initialize counters\n",
    "    attr_counts = defaultdict(Counter)\n",
    "    primary_counts = defaultdict(Counter)\n",
    "    coverage_stats = {}\n",
    "    \n",
    "    for attr_name in ATTRIBUTE_SCHEMA.keys():\n",
    "        attr_key = f\"attr_{attr_name}\"\n",
    "        primary_key = f\"attr_{attr_name}_primary\"\n",
    "        \n",
    "        # Count all extracted attributes\n",
    "        all_attrs = []\n",
    "        primary_attrs = []\n",
    "        \n",
    "        for ex in processed_examples:\n",
    "            all_attrs.extend(ex[attr_key])\n",
    "            if ex[primary_key]:\n",
    "                primary_attrs.append(ex[primary_key])\n",
    "        \n",
    "        attr_counts[attr_name] = Counter(all_attrs)\n",
    "        primary_counts[attr_name] = Counter(primary_attrs)\n",
    "        \n",
    "        # Coverage statistics\n",
    "        examples_with_attr = sum(1 for ex in processed_examples if ex[attr_key])\n",
    "        examples_with_primary = sum(1 for ex in processed_examples if ex[primary_key])\n",
    "        \n",
    "        coverage_stats[attr_name] = {\n",
    "            \"examples_with_attributes\": examples_with_attr,\n",
    "            \"examples_with_primary\": examples_with_primary,\n",
    "            \"coverage_percentage\": examples_with_attr / total_examples * 100,\n",
    "            \"primary_coverage_percentage\": examples_with_primary / total_examples * 100,\n",
    "            \"unique_attributes_found\": len(attr_counts[attr_name]),\n",
    "            \"total_attribute_mentions\": sum(attr_counts[attr_name].values())\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"coverage_stats\": coverage_stats,\n",
    "        \"attr_counts\": dict(attr_counts),\n",
    "        \"primary_counts\": dict(primary_counts),\n",
    "        \"total_examples\": total_examples\n",
    "    }\n",
    "\n",
    "# Analyze the sample\n",
    "quality_analysis = analyze_extraction_quality(processed_sample)\n",
    "\n",
    "print(\"\\nExtraction Quality Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "for attr_name, stats in quality_analysis[\"coverage_stats\"].items():\n",
    "    print(f\"\\n{attr_name.upper()}:\")\n",
    "    print(f\"  Coverage: {stats['examples_with_attributes']}/{quality_analysis['total_examples']} examples ({stats['coverage_percentage']:.1f}%)\")\n",
    "    print(f\"  Primary: {stats['examples_with_primary']}/{quality_analysis['total_examples']} examples ({stats['primary_coverage_percentage']:.1f}%)\")\n",
    "    print(f\"  Unique attributes found: {stats['unique_attributes_found']}\")\n",
    "    print(f\"  Total mentions: {stats['total_attribute_mentions']}\")\n",
    "    \n",
    "    # Show top attributes\n",
    "    top_attrs = quality_analysis[\"primary_counts\"][attr_name].most_common(5)\n",
    "    if top_attrs:\n",
    "        print(f\"  Top primary attributes: {', '.join([f'{attr}({count})' for attr, count in top_attrs])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Iterative Refinement\n",
    "\n",
    "Review extraction results and refine the attribute schema or extraction logic as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manual Review of 5 Extraction Results:\n",
      "============================================================\n",
      "\n",
      "Example 1:\n",
      "Text: Her sweater has long sleeves, cotton fabric and pure color patterns. The neckline of it is stand. This lady wears a three-point pants. The pants are with cotton fabric and floral patterns. The person is wearing leggings....\n",
      "Extracted attributes:\n",
      "  material: ['cotton'] (primary: cotton)\n",
      "  pattern: ['floral'] (primary: floral)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Text: The T-shirt this gentleman wears has short sleeves and it is with cotton fabric and striped patterns. The neckline of the T-shirt is lapel....\n",
      "Extracted attributes:\n",
      "  material: ['cotton'] (primary: cotton)\n",
      "  pattern: ['stripe'] (primary: stripe)\n",
      "  sleeve: ['short'] (primary: short)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Text: The person wears a sleeveless tank shirt with pure color patterns. The tank shirt is with cotton fabric. The pants the person wears is of three-point length. The pants are with cotton fabric and solid color patterns. There is an accessory on her wrist. There is an accessory in his her neck. There is...\n",
      "Extracted attributes:\n",
      "  material: ['cotton'] (primary: cotton)\n",
      "  pattern: ['pure color'] (primary: pure color)\n",
      "  sleeve: ['sleeveless'] (primary: sleeveless)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "Text: The sweater the female wears has long sleeves, its fabric is cotton, and it has pure color patterns. The sweater has a crew neckline. The female wears a short pants, with cotton fabric and pure color patterns. This lady is wearing a pair of socks....\n",
      "Extracted attributes:\n",
      "  material: ['cotton'] (primary: cotton)\n",
      "  pattern: ['pure color'] (primary: pure color)\n",
      "  neckline: ['crew', 'crew neck'] (primary: crew)\n",
      "  sleeve: ['long'] (primary: long)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "Text: The female is wearing a long-sleeve sweater with graphic patterns. The sweater is with chiffon fabric. It has a v-shape neckline....\n",
      "Extracted attributes:\n",
      "  material: ['chiffon'] (primary: chiffon)\n",
      "  pattern: ['graphic'] (primary: graphic)\n",
      "  sleeve: ['long'] (primary: long)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Manual review of extraction results\n",
    "def review_extractions(processed_examples: List[Dict], num_samples: int = 10) -> None:\n",
    "    \"\"\"\n",
    "    Manually review extraction results for quality assessment.\n",
    "    \n",
    "    Args:\n",
    "        processed_examples: List of processed examples\n",
    "        num_samples: Number of samples to review\n",
    "    \"\"\"\n",
    "    print(f\"\\nManual Review of {num_samples} Extraction Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Random sample for review\n",
    "    review_indices = random.sample(range(len(processed_examples)), min(num_samples, len(processed_examples)))\n",
    "    \n",
    "    for i, idx in enumerate(review_indices):\n",
    "        ex = processed_examples[idx]\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Text: {ex['text'][:300]}...\")\n",
    "        print(\"Extracted attributes:\")\n",
    "        \n",
    "        for attr_name in ATTRIBUTE_SCHEMA.keys():\n",
    "            attr_key = f\"attr_{attr_name}\"\n",
    "            primary_key = f\"attr_{attr_name}_primary\"\n",
    "            attrs = ex[attr_key]\n",
    "            primary = ex[primary_key]\n",
    "            if attrs or primary:\n",
    "                print(f\"  {attr_name}: {attrs} (primary: {primary})\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# Review sample extractions\n",
    "review_extractions(processed_sample, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Optimize Extraction Logic\n",
    "\n",
    "Based on the analysis, optimize the extraction logic and attribute schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing optimized extraction...\n",
      "\n",
      "Optimized Extraction Results:\n",
      "========================================\n",
      "material: 94.9% (was 94.9%, +0.0%)\n",
      "pattern: 75.5% (was 75.5%, +0.0%)\n",
      "neckline: 21.1% (was 21.1%, +0.0%)\n",
      "sleeve: 71.7% (was 71.7%, +0.0%)\n",
      "closure: 0.0% (was 0.0%, +0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Optimized extraction with improved patterns\n",
    "def extract_attributes_optimized(text: str, schema: Dict) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Optimized attribute extraction with better pattern matching.\n",
    "    \n",
    "    Args:\n",
    "        text: Product description text\n",
    "        schema: Attribute schema with keywords\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping attribute names to lists of found keywords\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    attributes = {}\n",
    "    \n",
    "    for attr_name, attr_config in schema.items():\n",
    "        found_keywords = []\n",
    "        \n",
    "        for keyword in attr_config[\"keywords\"]:\n",
    "            # More flexible matching - handle compound words and variations\n",
    "            keyword_lower = keyword.lower()\n",
    "            \n",
    "            # Exact word match\n",
    "            pattern = r'\\b' + re.escape(keyword_lower) + r'\\b'\n",
    "            if re.search(pattern, text_lower):\n",
    "                found_keywords.append(keyword)\n",
    "                continue\n",
    "            \n",
    "            # Handle compound keywords with spaces\n",
    "            if ' ' in keyword_lower:\n",
    "                # Allow some flexibility in spacing\n",
    "                flexible_pattern = r'\\b' + re.escape(keyword_lower).replace(' ', r'\\s+') + r'\\b'\n",
    "                if re.search(flexible_pattern, text_lower):\n",
    "                    found_keywords.append(keyword)\n",
    "                    continue\n",
    "            \n",
    "            # Handle common variations (e.g., \"t-shirt\" for \"tee\")\n",
    "            if attr_name == \"material\" and keyword_lower == \"cotton\":\n",
    "                if any(term in text_lower for term in [\"cotton\", \"cotton fabric\"]):\n",
    "                    found_keywords.append(keyword)\n",
    "            \n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_keywords = []\n",
    "        for keyword in found_keywords:\n",
    "            if keyword not in seen:\n",
    "                unique_keywords.append(keyword)\n",
    "                seen.add(keyword)\n",
    "        \n",
    "        attributes[attr_name] = unique_keywords\n",
    "    \n",
    "    return attributes\n",
    "\n",
    "# Update the process function to use optimized extraction\n",
    "def process_example_optimized(example: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Process example with optimized attribute extraction.\n",
    "    \"\"\"\n",
    "    text = example.get(\"text\", \"\")\n",
    "    \n",
    "    # Extract attributes with optimized function\n",
    "    extracted = extract_attributes_optimized(text, ATTRIBUTE_SCHEMA)\n",
    "    \n",
    "    # Add to example\n",
    "    for attr_name in ATTRIBUTE_SCHEMA.keys():\n",
    "        attr_key = f\"attr_{attr_name}\"\n",
    "        primary_key = f\"attr_{attr_name}_primary\"\n",
    "        \n",
    "        example[attr_key] = extracted[attr_name]\n",
    "        example[primary_key] = get_primary_attribute(extracted[attr_name])\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Test optimized extraction on same sample\n",
    "print(\"Testing optimized extraction...\")\n",
    "optimized_sample = [process_example_optimized(ex.copy()) for ex in sample_examples]\n",
    "\n",
    "# Compare results\n",
    "optimized_quality = analyze_extraction_quality(optimized_sample)\n",
    "print(\"\\nOptimized Extraction Results:\")\n",
    "print(\"=\" * 40)\n",
    "for attr_name in ATTRIBUTE_SCHEMA.keys():\n",
    "    old_stats = quality_analysis[\"coverage_stats\"][attr_name]\n",
    "    new_stats = optimized_quality[\"coverage_stats\"][attr_name]\n",
    "    \n",
    "    improvement = new_stats[\"coverage_percentage\"] - old_stats[\"coverage_percentage\"]\n",
    "    print(f\"{attr_name}: {new_stats['coverage_percentage']:.1f}% (was {old_stats['coverage_percentage']:.1f}%, +{improvement:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Enhanced Dataset\n",
    "\n",
    "What this block does:\n",
    "Provides code to process the full dataset (commented out for safety)\n",
    "Saves the enhanced dataset with extracted attributes\n",
    "Shows final summary and example extraction\n",
    "Key things to notice:\n",
    "Processes full dataset only when ready (commented by default)\n",
    "Saves in HuggingFace dataset format for easy loading\n",
    "Provides final verification of extraction quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Save Enhanced Dataset\n",
    "\n",
    "def save_enhanced_dataset():\n",
    "    \"\"\"\n",
    "    Process the full dataset and save with extracted attributes.\n",
    "    COMMENTED OUT FOR SAFETY - Uncomment only when ready to process full dataset.\n",
    "    \"\"\"\n",
    "    print(\"WARNING: This will process the entire dataset and may take significant time and resources.\")\n",
    "    print(\"Only run this when you're satisfied with the extraction quality.\")\n",
    "    \n",
    "    # Uncomment the following lines when ready:\n",
    "    \n",
    "    # print(\"Processing full dataset...\")\n",
    "    # full_processed = []\n",
    "    # for example in tqdm(train_ds, desc=\"Processing full dataset\"):\n",
    "    #     processed_example = process_example_optimized(example.copy())\n",
    "    #     full_processed.append(processed_example)\n",
    "    \n",
    "    # # Convert to Dataset\n",
    "    # enhanced_ds = Dataset.from_list(full_processed)\n",
    "    \n",
    "    # # Save to disk\n",
    "    # output_path = \"../data/processed/hf_with_attributes\"\n",
    "    # os.makedirs(output_path, exist_ok=True)\n",
    "    # enhanced_ds.save_to_disk(output_path)\n",
    "    # print(f\"Enhanced dataset saved to: {output_path}\")\n",
    "    \n",
    "    # return enhanced_ds\n",
    "    \n",
    "    return None\n",
    "\n",
    "# For demonstration, save the sample dataset instead\n",
    "def save_sample_enhanced_dataset():\n",
    "    \"\"\"\n",
    "    Save the processed sample dataset for testing.\n",
    "    \"\"\"\n",
    "    print(\"Saving enhanced sample dataset...\")\n",
    "    \n",
    "    # Create sample dataset\n",
    "    enhanced_sample_ds = Dataset.from_list(optimized_sample)\n",
    "    \n",
    "    # Save to disk\n",
    "    output_path = \"../data/processed/hf_with_attributes_sample\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    enhanced_sample_ds.save_to_disk(output_path)\n",
    "    print(f\"Enhanced sample dataset saved to: {output_path}\")\n",
    "    \n",
    "    return enhanced_sample_ds\n",
    "\n",
    "# Save sample dataset\n",
    "enhanced_sample = save_sample_enhanced_dataset()\n",
    "\n",
    "# Final summary and example extraction\n",
    "print(\"\\nFinal Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Sample dataset processed: {len(enhanced_sample)} examples\")\n",
    "print(\"\\nAttribute extraction coverage:\")\n",
    "\n",
    "for attr_name, stats in optimized_quality[\"coverage_stats\"].items():\n",
    "    print(f\"  {attr_name}: {stats['coverage_percentage']:.1f}% ({stats['examples_with_attributes']}/{optimized_quality['total_examples']} examples)\")\n",
    "\n",
    "# Show example extraction\n",
    "print(\"\\nExample Extraction:\")\n",
    "print(\"-\" * 30)\n",
    "example_idx = 0\n",
    "example = enhanced_sample[example_idx]\n",
    "print(f\"Text: {example['text'][:150]}...\")\n",
    "print(\"\\nExtracted attributes:\")\n",
    "for attr_name in ATTRIBUTE_SCHEMA.keys():\n",
    "    attr_key = f\"attr_{attr_name}\"\n",
    "    primary_key = f\"attr_{attr_name}_primary\"\n",
    "    attrs = example[attr_key]\n",
    "    primary = example[primary_key]\n",
    "    if attrs:\n",
    "        print(f\"  {attr_name}: {attrs} (primary: {primary})\")\n",
    "    else:\n",
    "        print(f\"  {attr_name}: None\")\n",
    "\n",
    "print(\"\\n✅ Enhanced dataset ready for use!\")\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Review extraction quality in validation step\")\n",
    "print(\"2. Uncomment and run full dataset processing when ready\")\n",
    "print(\"3. Use enhanced dataset for training similarity models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Validation and Error Analysis\n",
    "\n",
    "What this block does:\n",
    "Provides a framework for manual validation of extraction accuracy\n",
    "Randomly samples examples for human review\n",
    "Tracks validation metrics and issues\n",
    "Key things to notice:\n",
    "Uses reproducible random sampling\n",
    "Provides clear review guidelines for manual validation\n",
    "Tracks different types of errors (missing, incorrect, hallucinated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Validation and Error Analysis\n",
    "\n",
    "class AttributeValidator:\n",
    "    \"\"\"\n",
    "    Framework for validating attribute extraction accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, sample_size: int = 100, random_seed: int = 42):\n",
    "        \"\"\"\n",
    "        Initialize validator.\n",
    "        \n",
    "        Args:\n",
    "            dataset: Dataset with extracted attributes\n",
    "            sample_size: Number of examples to sample for validation\n",
    "            random_seed: Random seed for reproducible sampling\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.sample_size = min(sample_size, len(dataset))\n",
    "        self.random_seed = random_seed\n",
    "        self.validation_results = []\n",
    "        \n",
    "        # Sample examples for validation\n",
    "        random.seed(random_seed)\n",
    "        self.sample_indices = random.sample(range(len(dataset)), self.sample_size)\n",
    "        \n",
    "    def get_validation_sample(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get the validation sample for manual review.\n",
    "        \n",
    "        Returns:\n",
    "            List of examples for validation\n",
    "        \"\"\"\n",
    "        return [self.dataset[i] for i in self.sample_indices]\n",
    "    \n",
    "    def display_validation_guidelines(self):\n",
    "        \"\"\"\n",
    "        Display guidelines for manual validation.\n",
    "        \"\"\"\n",
    "        print(\"\\nVALIDATION GUIDELINES\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"For each example, evaluate the extracted attributes:\")\n",
    "        print(\"\\n1. MATERIAL:\")\n",
    "        print(\"   - Check if fabric types (cotton, denim, etc.) are correctly identified\")\n",
    "        print(\"   - Mark as MISSING if material mentioned but not extracted\")\n",
    "        print(\"   - Mark as INCORRECT if wrong material extracted\")\n",
    "        print(\"   - Mark as HALLUCINATED if material extracted but not in text\")\n",
    "        print(\"\\n2. PATTERN:\")\n",
    "        print(\"   - Verify patterns (stripe, floral, solid, etc.) are accurate\")\n",
    "        print(\"   - Pay attention to color-block vs solid color distinctions\")\n",
    "        print(\"\\n3. NECKLINE/SLEEVE:\")\n",
    "        print(\"   - Check if garment style attributes are correctly identified\")\n",
    "        print(\"   - Look for crew neck, v-neck, short/long sleeves, etc.\")\n",
    "        print(\"\\n4. CLOSURE:\")\n",
    "        print(\"   - Verify buttons, zippers, snaps are correctly detected\")\n",
    "        print(\"\\nRATING SCALE:\")\n",
    "        print(\"   CORRECT: All extracted attributes are accurate\")\n",
    "        print(\"   MOSTLY_CORRECT: >75% of attributes are accurate\")\n",
    "        print(\"   PARTIALLY_CORRECT: 25-75% of attributes are accurate\")\n",
    "        print(\"   POOR: <25% of attributes are accurate\")\n",
    "        print(\"\\nRecord your assessment for each example.\")\n",
    "    \n",
    "    def validate_sample_manually(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Interactive manual validation of the sample.\n",
    "        \n",
    "        Returns:\n",
    "            Validation results dictionary\n",
    "        \"\"\"\n",
    "        sample = self.get_validation_sample()\n",
    "        results = []\n",
    "        \n",
    "        print(f\"\\nStarting manual validation of {len(sample)} examples...\")\n",
    "        print(\"(This is a framework - in practice, you'd have human reviewers)\")\n",
    "        \n",
    "        # For demonstration, we'll do automated validation\n",
    "        # In real scenario, this would be manual\n",
    "        for i, example in enumerate(sample):\n",
    "            print(f\"\\nExample {i+1}/{len(sample)}:\")\n",
    "            print(f\"Text: {example['text'][:200]}...\")\n",
    "            \n",
    "            # Display extracted attributes\n",
    "            extracted_attrs = {}\n",
    "            for attr_name in ATTRIBUTE_SCHEMA.keys():\n",
    "                attr_key = f\"attr_{attr_name}\"\n",
    "                primary_key = f\"attr_{attr_name}_primary\"\n",
    "                attrs = example[attr_key]\n",
    "                primary = example[primary_key]\n",
    "                if attrs:\n",
    "                    extracted_attrs[attr_name] = attrs\n",
    "                    print(f\"  {attr_name}: {attrs} (primary: {primary})\")\n",
    "            \n",
    "            # Automated validation (in real scenario, this would be manual)\n",
    "            validation_result = self._automated_validation(example, extracted_attrs)\n",
    "            results.append(validation_result)\n",
    "            \n",
    "            # Show validation result\n",
    "            print(f\"  Validation: {validation_result['overall_rating']}\")\n",
    "            if validation_result['issues']:\n",
    "                print(f\"  Issues: {', '.join(validation_result['issues'])}\")\n",
    "            \n",
    "            # Stop after a few examples for demonstration\n",
    "            if i >= 4:\n",
    "                print(\"\\n... (showing first 5 examples for demonstration)\")\n",
    "                break\n",
    "        \n",
    "        return self._analyze_validation_results(results)\n",
    "    \n",
    "    def _automated_validation(self, example: Dict, extracted_attrs: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Automated validation for demonstration (in practice, this would be manual).\n",
    "        \"\"\"\n",
    "        text = example['text'].lower()\n",
    "        issues = []\n",
    "        correct_count = 0\n",
    "        total_attrs = len(extracted_attrs)\n",
    "        \n",
    "        for attr_name, attrs in extracted_attrs.items():\n",
    "            if not attrs:\n",
    "                continue\n",
    "                \n",
    "            # Check if extracted attributes appear in text\n",
    "            for attr in attrs:\n",
    "                if attr.lower() not in text:\n",
    "                    issues.append(f\"hallucinated_{attr_name}_{attr}\")\n",
    "                else:\n",
    "                    correct_count += 1\n",
    "        \n",
    "        # Determine overall rating\n",
    "        if total_attrs == 0:\n",
    "            rating = \"NO_ATTRIBUTES\"\n",
    "        elif issues:\n",
    "            rating = \"ISSUES_FOUND\"\n",
    "        else:\n",
    "            rating = \"APPEARS_CORRECT\"\n",
    "        \n",
    "        return {\n",
    "            \"example_id\": example.get(\"item_ID\", \"unknown\"),\n",
    "            \"overall_rating\": rating,\n",
    "            \"issues\": issues,\n",
    "            \"correct_attributes\": correct_count,\n",
    "            \"total_attributes\": total_attrs\n",
    "        }\n",
    "    \n",
    "    def _analyze_validation_results(self, results: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze validation results and compute metrics.\n",
    "        \n",
    "        Args:\n",
    "            results: List of validation result dictionaries\n",
    "        \n",
    "        Returns:\n",
    "            Analysis summary\n",
    "        \"\"\"\n",
    "        if not results:\n",
    "            return {}\n",
    "        \n",
    "        total_examples = len(results)\n",
    "        ratings = Counter(r['overall_rating'] for r in results)\n",
    "        \n",
    "        # Collect all issues\n",
    "        all_issues = []\n",
    "        for r in results:\n",
    "            all_issues.extend(r['issues'])\n",
    "        \n",
    "        issue_counts = Counter(all_issues)\n",
    "        \n",
    "        # Calculate accuracy metrics\n",
    "        total_attributes = sum(r['total_attributes'] for r in results)\n",
    "        correct_attributes = sum(r['correct_attributes'] for r in results)\n",
    "        \n",
    "        accuracy = correct_attributes / total_attributes if total_attributes > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"total_examples_validated\": total_examples,\n",
    "            \"rating_distribution\": dict(ratings),\n",
    "            \"common_issues\": dict(issue_counts.most_common(10)),\n",
    "            \"attribute_accuracy\": accuracy,\n",
    "            \"total_attributes_checked\": total_attributes,\n",
    "            \"correct_attributes\": correct_attributes\n",
    "        }\n",
    "\n",
    "# Initialize validator\n",
    "validator = AttributeValidator(enhanced_sample, sample_size=100)\n",
    "\n",
    "# Display guidelines\n",
    "validator.display_validation_guidelines()\n",
    "\n",
    "# Run validation\n",
    "validation_results = validator.validate_sample_manually()\n",
    "\n",
    "# Display validation summary\n",
    "print(\"\\nVALIDATION SUMMARY\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Examples validated: {validation_results['total_examples_validated']}\")\n",
    "print(f\"Attribute accuracy: {validation_results['attribute_accuracy']:.1%}\")\n",
    "print(f\"Total attributes checked: {validation_results['total_attributes_checked']}\")\n",
    "print(f\"Correct attributes: {validation_results['correct_attributes']}\")\n",
    "\n",
    "print(\"\\nRating distribution:\")\n",
    "for rating, count in validation_results['rating_distribution'].items():\n",
    "    pct = count / validation_results['total_examples_validated'] * 100\n",
    "    print(f\"  {rating}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nMost common issues:\")\n",
    "for issue, count in list(validation_results['common_issues'].items())[:5]:\n",
    "    print(f\"  {issue}: {count}\")\n",
    "\n",
    "# Recommendations based on validation\n",
    "accuracy = validation_results['attribute_accuracy']\n",
    "if accuracy > 0.8:\n",
    "    print(\"\\n✅ High accuracy! Ready for production use.\")\n",
    "elif accuracy > 0.6:\n",
    "    print(\"\\n⚠️ Moderate accuracy. Consider refining extraction rules.\")\n",
    "else:\n",
    "    print(\"\\n❌ Low accuracy. Significant improvements needed.\")\n",
    "    print(\"   Consider:\")\n",
    "    print(\"   - Adding more keywords to schema\")\n",
    "    print(\"   - Implementing more sophisticated matching\")\n",
    "    print(\"   - Using machine learning for extraction\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Address identified issues\")\n",
    "print(\"2. Re-run validation after improvements\")\n",
    "print(\"3. Proceed to model training when accuracy meets requirements\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
